{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMva1BNXR3qL3BWIZLwxcB1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isavida/football-task/blob/team-classification/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics -q\n",
        "!pip install easyocr -q"
      ],
      "metadata": {
        "id": "keemGl--AKn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac8c634-393b-403e-f886-8c06daa423e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.3/719.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import colorsys\n",
        "import copy\n",
        "import cv2\n",
        "import easyocr\n",
        "import imutils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.cluster import KMeans\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "bVbutFMSAMle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "dIh51UQTLLUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def git_setup(token):\n",
        "  os.system(f\"git config --global user.email 'isabel.vidaurre@hotmail.com'\")\n",
        "  os.system(f\"git config --global user.name 'isavida'\")\n",
        "  os.system(f\"git clone https://{token}@github.com/isavida/football-task.git\")\n",
        "  os.chdir(\"football-task\")"
      ],
      "metadata": {
        "id": "986DSJyLhX2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git_setup('123123')"
      ],
      "metadata": {
        "id": "RXhyzdLKnHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The task consists of taking as input a video footage and output:\n",
        "\n",
        "\n",
        "*   A JSONL file that contains a row for each 5th frame with the number of players from home-team and away-team, numbers of referees and position of the ball.\n",
        "*   A video with the team players with the bounding boxes of detections\n",
        "classified according to their team and the annotation of the ball when detected\n",
        "\n",
        "\n",
        "It is decided to do the player detection task first, as the data for the jsonl file can be obtained from these detections.\n",
        "\n"
      ],
      "metadata": {
        "id": "fyv_IupWLbBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Players detection"
      ],
      "metadata": {
        "id": "jA1cwRbrI5sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_capture = \"clip_3\""
      ],
      "metadata": {
        "id": "lPfaE9q9AsSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b69204f-b185-4dd0-ab7e-35e145a9656c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING ⚠️ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "Results saved to \u001b[1mruns/detect/clip_3_result\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Team classification\n",
        "To correctly classify each team, the team colors should be finded.\n",
        "\n",
        "As a first approach, is proposed to use the **k-means clustering algorithm** to detect the colors of the jerseys from the bounding boxes of the players, processing the image by removing the green background to have the maximum percentage of relevant information possible.\n",
        "\n",
        "This approach does not provide the expected results and also does not provide the information of which equipment belongs to the home team and which to the visiting team, so it was decided to obtain all this information from the **scoreboard**.\n",
        "\n",
        "*Although this notebook does not keep all the code that has been tested for readability, it can be consulted in previous commits.*"
      ],
      "metadata": {
        "id": "5kM3EsqlP7gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting info from scoreboard\n"
      ],
      "metadata": {
        "id": "jf_thpL1Siy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering that in the task we should distinguish between the home and the away team, another approach could be to identify the colors and the affiliation to the home or the away team by looking at the scoreboard."
      ],
      "metadata": {
        "id": "c2SaueJ3Szkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_static_pixels_from_video(filepath, n_samples=10, std_ratio = 0.04, crop_x_ratio = 0.4, crop_y_ratio = 0.2):\n",
        "    ''' Crop parameters just accelerates the workflow since we know that the\n",
        "    scoreboard is located at the upper-left corner. The reader can test\n",
        "    this function with both crop ratios = 1, which takes around 30 secs using CPU'''\n",
        "    cap = cv2.VideoCapture(filepath)\n",
        "\n",
        "    # Randomly select n sample frames\n",
        "    sample_frames_index = [np.random.randint(0, cap.get(cv2.CAP_PROP_FRAME_COUNT)) for i in range(n_samples)]\n",
        "\n",
        "    # Store selected frames in an array\n",
        "    sample_frames = []\n",
        "    for sfi in sample_frames_index:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, sfi)\n",
        "        _, frame = cap.read()\n",
        "        if frame is not None:\n",
        "            sample_frames.append(frame[0:int(crop_y_ratio * frame.shape[0]),\n",
        "                                0:int(crop_x_ratio * frame.shape[1])])\n",
        "\n",
        "    # std will help to check static pixels\n",
        "    # median obtains a precise scoreboard in case it's damaged on any frame\n",
        "    std_frames = np.std(sample_frames, axis=0).astype(dtype=np.uint8)\n",
        "    median_frames = np.median(sample_frames, axis=0).astype(dtype=np.uint8)\n",
        "\n",
        "    # get mean over color channels\n",
        "    std_frame_mean = np.mean(std_frames/255, axis=2)\n",
        "    std_frame_mean_3D = np.repeat(std_frame_mean[:,:,np.newaxis], 3, axis=2)\n",
        "\n",
        "    # filter static pixels\n",
        "    background = np.where(std_frame_mean_3D < std_ratio, median_frames, 0)\n",
        "\n",
        "    return background\n",
        "\n",
        "def xywh_from_points_with_scale(points_2d, scale=1.2):\n",
        "    ''' Compute center_x, center_y, width and weight given N 2d points '''\n",
        "\n",
        "    x_min = np.min(points_2d[:,0], axis=0)\n",
        "    x_max = np.max(points_2d[:,0], axis=0)\n",
        "    y_min = np.min(points_2d[:,1], axis=0)\n",
        "    y_max = np.max(points_2d[:,1], axis=0)\n",
        "\n",
        "    return [(x_max+x_min)/2,\n",
        "            (y_max+y_min)/2,\n",
        "            (x_max-x_min) * scale,\n",
        "            (y_max-y_min) * scale]\n",
        "\n",
        "def crop_image_given_xywh(image, xywh):\n",
        "    x_min = int(xywh[0] - xywh[2]/2)\n",
        "    x_max = int(xywh[0] + xywh[2]/2)\n",
        "    y_min = int(xywh[1] - xywh[3]/2)\n",
        "    y_max = int(xywh[1] + xywh[3]/2)\n",
        "\n",
        "    return image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "def detect_team_scoreboard_and_crop_image(background):\n",
        "    # read image\n",
        "    img = copy.deepcopy(background)\n",
        "\n",
        "    # instance text detector\n",
        "    reader = easyocr.Reader(['en'], gpu=False)\n",
        "\n",
        "    # detect text on image\n",
        "    text_ = reader.readtext(img)\n",
        "\n",
        "    threshold = 0.25\n",
        "    initials = []\n",
        "\n",
        "    # draw bbox and text of team initials\n",
        "    for t_, t in enumerate(text_):\n",
        "        bbox, text, score = t\n",
        "\n",
        "        if score > threshold and len(text) == 3:\n",
        "            #cv2.rectangle(img, bbox[0], bbox[2], (0, 255, 0), 5)\n",
        "            #cv2.putText(img, text, bbox[0], cv2.FONT_HERSHEY_COMPLEX, 0.65, (255, 0, 0), 2)\n",
        "\n",
        "            initials.append(bbox)\n",
        "\n",
        "    initials_np = np.array(initials)\n",
        "    initials_np = initials_np.reshape(initials_np.shape[0] * initials_np.shape[1], initials_np.shape[2])\n",
        "\n",
        "    xywh = xywh_from_points_with_scale(initials_np)\n",
        "    crop_img = crop_image_given_xywh(img, xywh)\n",
        "\n",
        "    return crop_img\n"
      ],
      "metadata": {
        "id": "nY1Wk5SWwvYG"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_scoreboard_per_team(scoreboard):\n",
        "    img_width = scoreboard.shape[1]\n",
        "    return scoreboard[:,:img_width//2,:], scoreboard[:,img_width//2:,:]"
      ],
      "metadata": {
        "id": "uxv0yEnsvwVa"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_img(img, K=32):\n",
        "    ''' This function quantize the input image by using k-means algorithm,\n",
        "        dividing the input image in the K most-predominant colors'''\n",
        "    # Preprocess input img\n",
        "    Z = img.reshape((-1,3))\n",
        "    Z = np.float32(Z)\n",
        "\n",
        "    # Specify stopping criteria, max_iters and desired-accuracy\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "    #(samples,nclusters,None,criteria,attempts,flags)\n",
        "    ret,label,center=cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "    # Postprocess back img to original structure\n",
        "    center = np.uint8(center)\n",
        "    res = center[label.flatten()]\n",
        "    quantized_img = res.reshape((img.shape))\n",
        "\n",
        "    return quantized_img"
      ],
      "metadata": {
        "id": "cpzc8cnkjaDu"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_dominant_colors(dominant_colors):\n",
        "    # Create a blank white image\n",
        "    bar = np.zeros((50, 300, 3), dtype=np.uint8)\n",
        "    startX = 0\n",
        "\n",
        "    # For each dominant color, draw a rectangle on the blank image\n",
        "    for color in dominant_colors:\n",
        "        endX = startX + (300 // len(dominant_colors))\n",
        "        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50), color.astype(int).tolist(), -1)\n",
        "        startX = endX\n",
        "\n",
        "    # Display the image\n",
        "    cv2_imshow(bar)"
      ],
      "metadata": {
        "id": "GT-7edq4CJeh"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_color_frequencies(img):\n",
        "    ''' Get color frequencies and return pd dataframe '''\n",
        "\n",
        "    img_nx3 = np.float32(img.reshape(-1, 3))\n",
        "\n",
        "    unique_pixels, counts = np.unique(img_nx3, axis=0, return_counts=True)\n",
        "\n",
        "    color_dict = {'color': [tuple(color) for color in unique_pixels],\n",
        "                  'frequency': counts}\n",
        "\n",
        "    return pd.DataFrame(color_dict)\n",
        "\n",
        "def get_most_distinctive_color(color_freq_home, color_freq_away, ratio=10):\n",
        "  df_merged = pd.merge(color_freq_home, color_freq_away, on='color', suffixes=('_df1', '_df2'), how='outer')\n",
        "\n",
        "  # Compute difference between frequencies in both dataframes\n",
        "  df_merged['frequency_difference'] = abs(df_merged['frequency_df1']/df_merged['frequency_df2'])\n",
        "\n",
        "  # If the frequencies are bigger than ratio or lower than 1/ratio or Nan(meaning that the color is in one side, but not in the other) is considered a difference to consider\n",
        "  outstanding_differences = df_merged[\n",
        "    (df_merged['frequency_difference'] > ratio) |\n",
        "    (df_merged['frequency_difference'] < 1/ratio) |\n",
        "    (df_merged['frequency_difference'].isna())\n",
        "  ]\n",
        "\n",
        "  # Retrieve the rows with the most frequency for each df from these differences\n",
        "  max_frequency_df1 = outstanding_differences.loc[outstanding_differences['frequency_df1'].idxmax()]\n",
        "  max_frequency_df2 = outstanding_differences.loc[outstanding_differences['frequency_df2'].idxmax()]\n",
        "\n",
        "  return max_frequency_df1['color'], max_frequency_df2['color']\n"
      ],
      "metadata": {
        "id": "c4VtFhBqf-je"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_color_per_team_from_video(filepath, kmeans_nclusters=32, color_freq_ratio=10):\n",
        "    ''' Workflow which takes a video as input and return the color assigned to home and away team '''\n",
        "\n",
        "    # Extract scoreboard from video\n",
        "    background = get_static_pixels_from_video(filepath)\n",
        "    scoreboard = detect_team_scoreboard_and_crop_image(background)\n",
        "\n",
        "    # Quantize scoreboard and split in home-away teams\n",
        "    quantized_scoreboard = quantize_img(scoreboard, kmeans_nclusters)\n",
        "    quantized_home_scoreboard, quantized_away_scoreboard = split_scoreboard_per_team(quantized_scoreboard)\n",
        "\n",
        "    # Get color frequency per team\n",
        "    color_frequencies_home_scoreboard = get_color_frequencies(quantized_home_scoreboard)\n",
        "    color_frequencies_away_scoreboard = get_color_frequencies(quantized_away_scoreboard)\n",
        "\n",
        "    # Get the most used color in a team that is mostly never used in the other team\n",
        "    return get_most_distinctive_color(color_frequencies_home_scoreboard, color_frequencies_away_scoreboard, color_freq_ratio)"
      ],
      "metadata": {
        "id": "rwNT01HB7UeW"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = f\"data/{video_capture}.mp4\"\n",
        "home_color, away_color = get_color_per_team_from_video(filepath)\n",
        "\n",
        "print(home_color, away_color)\n",
        "display_dominant_colors([np.array(home_color), np.array(away_color)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "hPwD3D6L9ok0",
        "outputId": "a2000f70-0bde-484f-faef-da1fda31aaa5"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Using CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(34.0, 30.0, 172.0) (126.0, 58.0, 41.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=300x50>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAAyCAIAAABptHF9AAAAtUlEQVR4nO3TMQ2AMAAAQUAHYagurNQRIhHB8Gm4U/DL7881NlY2z7tO4JOjDoC/MyHETAgxE0LMhBAzIcRMCDETQsyEEDMhxEwIMRNCzIQQMyHETAgxE0LMhBAzIcRMCDETQsyEEDMhxEwIMRNCzIQQMyHETAgxE0LMhBAzIcRMCDETQsyEEDMhxEwIMRNCzIQQMyHETAgxE0LMhBAzIcRMCDETQsyEEDMhxEwIMRNCzIQQewEXGwJFEIwRdgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Players classification\n",
        "Once we have obtained from the scoreboard the color of the home team and the color of the away team, we use the OpenCV *cv2.inRange* method to detect the colors in the jerseys that are in a range around the colors of the teams.\n",
        "\n",
        "\n",
        "This process is performed in the HSV color space, as it allows a better color separation and to take into account aspects such as brightness and saturation, which are important for color detection."
      ],
      "metadata": {
        "id": "0MnmPm7xF4dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_color_boundaries(color):\n",
        "  h, s, v= colorsys.rgb_to_hsv(color[2], color[1], color[0]) #obtain hsv values from color\n",
        "  # translate into opencv space\n",
        "  h = int(180*h)\n",
        "  s = int(255*s)\n",
        "  v = int(v)\n",
        "\n",
        "  low_h=h-20\n",
        "  top_h=h+20\n",
        "  additionalMask = False\n",
        "\n",
        "  # h coordinates are circular, we have to consider this for red color\n",
        "  if low_h < 0:\n",
        "    new_low_h = low_h + 180\n",
        "    new_top_h = 180\n",
        "    low_h = 0\n",
        "    additionalMask = True\n",
        "  elif top_h>180:\n",
        "    new_top_h = top_h - 180\n",
        "    new_low_h = 0\n",
        "    top_h = 180\n",
        "    additionalMask = True\n",
        "\n",
        "  lower_bound = np.array([low_h, np.clip(s-90,0,255), np.clip(v-90,0,255)])\n",
        "  upper_bound = np.array([top_h, np.clip(s+90,0,255), np.clip(v+90,0,255)])\n",
        "\n",
        "  if additionalMask:\n",
        "    additional_lower_bound = np.array([new_low_h, np.clip(s-90,0,255), np.clip(v-90,0,255)])\n",
        "    additional_upper_bound = np.array([new_top_h, np.clip(s+90,0,255), np.clip(v+90,0,255)])\n",
        "    return lower_bound, upper_bound, additional_lower_bound, additional_upper_bound\n",
        "\n",
        "  return lower_bound, upper_bound\n",
        "\n",
        "def get_mask(player_hsv, boundaries):\n",
        "\n",
        "  mask = cv2.inRange(player_hsv, boundaries[0], boundaries[1])\n",
        "\n",
        "  if len(boundaries)==4:\n",
        "    mask1 = cv2.inRange(player_hsv, boundaries[2], boundaries[3])\n",
        "    mask = mask + mask1\n",
        "\n",
        "  return mask\n",
        "\n",
        "def team_classification(player, ht_color_boundaries, at_color_boundaries, debug=False):\n",
        "\n",
        "  player_hsv = cv2.cvtColor(player,cv2.COLOR_BGR2HSV)\n",
        "\n",
        "  output_ht = cv2.bitwise_and(player_hsv,player_hsv,mask=get_mask(player_hsv, ht_color_boundaries))\n",
        "  ht_count = np.count_nonzero(output_ht)\n",
        "\n",
        "  output_at = cv2.bitwise_and(player_hsv,player_hsv,mask=get_mask(player_hsv, at_color_boundaries))\n",
        "  at_count = np.count_nonzero(output_at)\n",
        "\n",
        "  tot_count = np.count_nonzero(player_hsv)\n",
        "\n",
        "  ht_percentage = ht_count/tot_count\n",
        "  at_percentage = at_count/tot_count\n",
        "\n",
        "  if debug:\n",
        "    cv2_imshow(player_hsv)\n",
        "\n",
        "    print('Percentage of pixels home team : ', ht_percentage)\n",
        "    cv2_imshow(output_ht)\n",
        "\n",
        "    print('Percentage of pixels away team  : ', at_percentage)\n",
        "    cv2_imshow(output_at)\n",
        "\n",
        "\n",
        "  if ht_percentage>0.01 and ht_percentage>at_percentage:\n",
        "    return 'Home', home_color\n",
        "\n",
        "  elif at_percentage>0.01 and at_percentage>ht_percentage:\n",
        "    return 'Away', away_color\n",
        "\n",
        "  else:\n",
        "    return 'Not sure', (0.0, 0.0, 0.0)\n"
      ],
      "metadata": {
        "id": "K54C_-F8NzAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_upper_left_corner_location(coordinates):\n",
        "  ''' YOLO x-y coordinates refers to the center of the detected image.\n",
        "  This functions gets the upper-left corner as a util in order to create the mask of the bounding boxes '''\n",
        "  center_x, center_y, w, h = coordinates\n",
        "  x= int(center_x - w/2)\n",
        "  y= int(center_y - h/2)\n",
        "\n",
        "  return x, y\n"
      ],
      "metadata": {
        "id": "ziZxKn2HVhta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_processed_img(orig_img, bounding_boxes, ht_boundaries, at_boundaries):\n",
        "  ## remove green\n",
        "  hsv = cv2.cvtColor(orig_img, cv2.COLOR_BGR2HSV)\n",
        "  green_mask = cv2.inRange(hsv, (35, 35, 35), (70, 255,255))\n",
        "  inverted_green_mask= cv2.bitwise_not(green_mask)\n",
        "  processed_img = cv2.bitwise_and(orig_img,orig_img, mask=inverted_green_mask)\n",
        "  final_image = np.copy(orig_img)\n",
        "  for box in bounding_boxes:\n",
        "    if box.cls==0: #cls 0 corresponds to person\n",
        "      x, y, w, h=map(int, box.xywh.tolist()[0])\n",
        "      x,y = get_upper_left_corner_location([int(x),y,w,h])\n",
        "\n",
        "      player_torso = processed_img[y:y+(h//2),x:x+w] #crop image to only have the torso\n",
        "      team_text, color_float = team_classification(player_torso, ht_boundaries, at_boundaries, debug=False)\n",
        "      final_image = cv2.rectangle (final_image, (x, y), (x+w,y+h), (int(color_float[0]), int(color_float[1]), int(color_float[2])), 2)\n",
        "      final_image = cv2.putText(final_image, team_text,(x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.50, (0, 0, 0), 2)\n",
        "\n",
        "  return final_image"
      ],
      "metadata": {
        "id": "7FFNa-IFvDmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "input_video = cv2.VideoCapture(f'data/{video_capture}.mp4')\n",
        "width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(input_video.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "if not os.path.exists('output'):\n",
        "  os.makedirs('output')\n",
        "\n",
        "#create video output with same properties as input\n",
        "output_video = cv2.VideoWriter(f'output/{video_capture}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "ht_boundaries = get_color_boundaries(home_color)\n",
        "at_boundaries = get_color_boundaries(away_color)\n",
        "\n",
        "while input_video.isOpened():\n",
        "  success, frame = input_video.read()\n",
        "  if success:\n",
        "    results = model.track(frame, persist = True, verbose=False) # predict each frame with YOLO model\n",
        "    for i, r in enumerate(results):\n",
        "      new_frame = get_processed_img(r.orig_img, r.boxes, ht_boundaries, at_boundaries)\n",
        "      output_video.write(new_frame)\n",
        "  else:\n",
        "    break\n",
        "\n",
        "input_video.release()\n",
        "output_video.release()\n"
      ],
      "metadata": {
        "id": "bbDoBw8JR9z0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e42a574b-627a-41b2-dcb8-bb32992c3300"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for i, r in enumerate(results):\\n  print(i)\\n  new_frame = get_processed_img(r.orig_img, r.boxes, ht_boundaries, at_boundaries)\\n  output_video.write(new_frame)\\noutput_video.release()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    }
  ]
}
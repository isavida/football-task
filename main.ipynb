{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isavida/football-task/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keemGl--AKn3",
        "outputId": "df54d7b3-29d7-401a-d70f-6650dc4d6321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install this specific YOLO branch which includes weighted loss function\n",
        "!git clone --branch fix#8578 https://github.com/hulkds/ultralytics.git -q\n",
        "!pip install /content/ultralytics/ lapx==0.5.5 -q\n",
        "\n",
        "# Fine-tuning dataset\n",
        "!wget -q -O dataset.zip https://universe.roboflow.com/ds/91Soi5QkdU?key=E6tIgxhinz\n",
        "!unzip -q dataset.zip -d dataset\n",
        "\n",
        "# Download drive folder\n",
        "!pip install -U --no-cache-dir gdown --pre -q\n",
        "!gdown --id 1AXgq-cQtfJdeinnDD8mJQAN26HsJWE99 -O task.zip -q\n",
        "!unzip -q task.zip\n",
        "\n",
        "!pip install easyocr -q\n",
        "!pip install jsonlines -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVbutFMSAMle"
      },
      "outputs": [],
      "source": [
        "import colorsys\n",
        "import copy\n",
        "import cv2\n",
        "import easyocr\n",
        "import imutils\n",
        "import jsonlines\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.cluster import KMeans\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.data.augment import Albumentations\n",
        "from ultralytics.utils import LOGGER, colorstr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Augmentation"
      ],
      "metadata": {
        "id": "os4a6qbXvaXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, p=1.0):\n",
        "        \"\"\"Initialize the transform object for YOLO bbox formatted params.\"\"\"\n",
        "        self.p = p\n",
        "        self.transform = None\n",
        "        prefix = colorstr(\"albumentations: \")\n",
        "        try:\n",
        "            import albumentations as A\n",
        "\n",
        "            # check_version(A.__version__, \"1.0.3\", hard=True)  # version requirement\n",
        "\n",
        "            # Transforms\n",
        "            T = [\n",
        "                A.MotionBlur(p=0.8),\n",
        "                A.Affine(scale=0.8, rotate = [-15,15], shear=[-30,30], p=0.5, mode=cv2.BORDER_REFLECT),\n",
        "                A.Blur(p=0.3),\n",
        "                A.GaussNoise(p=0.3),\n",
        "                A.RandomBrightnessContrast(p=0.3),\n",
        "                A.GridDistortion(p=0.2),\n",
        "                A.RandomGamma(p=0.2)\n",
        "            ]\n",
        "            self.transform = A.Compose(T, bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]))\n",
        "\n",
        "            LOGGER.info(prefix + \", \".join(f\"{x}\".replace(\"always_apply=False, \", \"\") for x in T if x.p))\n",
        "        except ImportError:  # package not installed, skip\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f\"{prefix}{e}\")\n",
        "\n",
        "Albumentations.__init__ = __init__\n"
      ],
      "metadata": {
        "id": "DSYJ4Sihve8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define inference and fine-tuning"
      ],
      "metadata": {
        "id": "_9mGWK76zeon"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKg1dQLXc1lq"
      },
      "outputs": [],
      "source": [
        "def finetune_model(model, datapath, pos_weight, imgsz=640, epochs=100, patience=10):\n",
        "    return model.train(data=datapath, pos_weight=pos_weight, imgsz=imgsz, epochs=epochs, patience=patience, dropout=0.2)\n",
        "\n",
        "def inference_video(model, filename, persist=False, classes=[0,1,2]):\n",
        "    return model.track(source=filename, save = False, conf=0.1, persist=persist, verbose = False, classes=classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA1cwRbrI5sI"
      },
      "source": [
        "#Players detection and team classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf_thpL1Siy5"
      },
      "source": [
        "## Getting colors from scoreboard\n",
        "\n",
        "To correctly classify each player into their team, the team colors should be finded.\n",
        "\n",
        "As a first approach, is proposed to use the **k-means clustering algorithm** to detect the colors of the jerseys from the bounding boxes of the players, processing the image by removing the green background to have the maximum percentage of relevant information possible.\n",
        "\n",
        "This approach does not provide the expected results and also does not provide the information of which equipment belongs to the home team and which to the visiting team, so it was decided to obtain all this information from the **scoreboard**.\n",
        "\n",
        "*Although this notebook does not keep all the code that has been tested for readability, it can be consulted in previous commits.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqklomVPjWG2"
      },
      "source": [
        "###Scoreboard recognition\n",
        "The first step to obtain the scoreboard colors is to recognize the scoreboard itself. To do this, the pixels that remain **static** in several random frames are obtained and a **letter detector** is used to cut out the rectangle containing the scoreboard from the letters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY1Wk5SWwvYG"
      },
      "outputs": [],
      "source": [
        "def get_static_pixels_from_video(filepath, n_samples=100, std_ratio = 0.04, crop_x_ratio = 0.4, crop_y_ratio = 0.2):\n",
        "    ''' Crop parameters just accelerates the workflow since we know that the\n",
        "    scoreboard is located at the upper-left corner. The reader can test\n",
        "    this function with both crop ratios = 1, which takes around 30 secs using CPU'''\n",
        "    cap = cv2.VideoCapture(filepath)\n",
        "\n",
        "    # Randomly select n sample frames\n",
        "    sample_frames_index = [np.random.randint(0, cap.get(cv2.CAP_PROP_FRAME_COUNT)) for i in range(n_samples)]\n",
        "\n",
        "    # Store selected frames in an array\n",
        "    sample_frames = []\n",
        "    for sfi in sample_frames_index:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, sfi)\n",
        "        _, frame = cap.read()\n",
        "        if frame is not None:\n",
        "            sample_frames.append(frame[0:int(crop_y_ratio * frame.shape[0]),\n",
        "                                0:int(crop_x_ratio * frame.shape[1])])\n",
        "\n",
        "    # std will help to check static pixels\n",
        "    # median obtains a precise scoreboard in case it's damaged on any frame\n",
        "    std_frames = np.std(sample_frames, axis=0).astype(dtype=np.uint8)\n",
        "    median_frames = np.median(sample_frames, axis=0).astype(dtype=np.uint8)\n",
        "\n",
        "    # get mean over color channels\n",
        "    std_frame_mean = np.mean(std_frames/255, axis=2)\n",
        "    std_frame_mean_3D = np.repeat(std_frame_mean[:,:,np.newaxis], 3, axis=2)\n",
        "\n",
        "    # filter static pixels\n",
        "    background = np.where(std_frame_mean_3D < std_ratio, median_frames, 0)\n",
        "\n",
        "    return background\n",
        "\n",
        "def xywh_from_points_with_scale(points_2d, scale=1.2):\n",
        "    ''' Compute center_x, center_y, width and weight given N 2d points '''\n",
        "    x_min = np.min(points_2d[:,0], axis=0)\n",
        "    x_max = np.max(points_2d[:,0], axis=0)\n",
        "    y_min = np.min(points_2d[:,1], axis=0)\n",
        "    y_max = np.max(points_2d[:,1], axis=0)\n",
        "\n",
        "    return [(x_max+x_min)/2,\n",
        "            (y_max+y_min)/2,\n",
        "            (x_max-x_min) * scale,\n",
        "            (y_max-y_min) * scale]\n",
        "\n",
        "def crop_image_given_xywh(image, xywh):\n",
        "    ''' Crop the input image based on given bounding box coordinates in xywh format '''\n",
        "    x_min = int(xywh[0] - xywh[2]/2)\n",
        "    x_max = int(xywh[0] + xywh[2]/2)\n",
        "    y_min = int(xywh[1] - xywh[3]/2)\n",
        "    y_max = int(xywh[1] + xywh[3]/2)\n",
        "\n",
        "    return image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "def detect_team_scoreboard_and_crop_image(background):\n",
        "    ''' Detect team initials on a scoreboard image and crop the image around the detected initials '''\n",
        "    img = copy.deepcopy(background)\n",
        "\n",
        "    # Initialize the text detector\n",
        "    reader = easyocr.Reader(['en'], gpu=True)\n",
        "\n",
        "    # Detect text on the image\n",
        "    text_ = reader.readtext(img)\n",
        "\n",
        "    # Set threshold for text detection confidence\n",
        "    threshold = 0.25\n",
        "    initials = []\n",
        "\n",
        "    # Iterate over detected text and check if the text meets the criteria for team initials\n",
        "    for t_, t in enumerate(text_):\n",
        "        bbox, text, score = t\n",
        "\n",
        "        if score > threshold and len(text) == 3 and ~(any(char.isdigit() for char in text)):\n",
        "            initials.append(bbox)\n",
        "\n",
        "    # Convert bounding box points to xywh format and crop image based on the detected bounding boxes\n",
        "    initials_np = np.array(initials)\n",
        "    initials_np = initials_np.reshape(initials_np.shape[0] * initials_np.shape[1], initials_np.shape[2])\n",
        "    xywh = xywh_from_points_with_scale(initials_np)\n",
        "    crop_img = crop_image_given_xywh(img, xywh)\n",
        "\n",
        "    return crop_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxv0yEnsvwVa"
      },
      "outputs": [],
      "source": [
        "def split_scoreboard_per_team(scoreboard):\n",
        "    ''' Split image in 2 by x = w//2'''\n",
        "    img_width = scoreboard.shape[1]\n",
        "    return scoreboard[:,:img_width//2,:], scoreboard[:,img_width//2:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdB3jSqcmD8m"
      },
      "source": [
        "### Color extraction\n",
        "The second step is to obtain the colors of the scoreboard (the one on the left belongs to the home team and the one on the right belongs to the away team).\n",
        "\n",
        "To do this, we use the k-means clustering algorithm to keep the main colors of the scoreboard. This **quantization** groups the similar colors and facilitates the subsequent comparison. We use to our advantage in this approach the fact that teams wear colors that are easily distinguishable from each other when playing a match.\n",
        "\n",
        "Once this quantization is done, we divide the scoreboard in half and compare the **frequency** of each color on each side to identify which colors are the most distinctive on each side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpzc8cnkjaDu"
      },
      "outputs": [],
      "source": [
        "def quantize_img(img, K=32):\n",
        "    ''' This function quantize the input image by using k-means algorithm,\n",
        "        dividing the input image in the K most-predominant colors'''\n",
        "    # Preprocess input img\n",
        "    Z = img.reshape((-1,3))\n",
        "    Z = np.float32(Z)\n",
        "\n",
        "    # Specify stopping criteria, max_iters and desired-accuracy\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "    #(samples,nclusters,None,criteria,attempts,flags)\n",
        "    ret,label,center=cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "    # Postprocess back img to original structure\n",
        "    center = np.uint8(center)\n",
        "    res = center[label.flatten()]\n",
        "    quantized_img = res.reshape((img.shape))\n",
        "\n",
        "    return quantized_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT-7edq4CJeh"
      },
      "outputs": [],
      "source": [
        "def display_colors(colors):\n",
        "    # Create a blank white image\n",
        "    bar = np.zeros((50, 300, 3), dtype=np.uint8)\n",
        "    startX = 0\n",
        "\n",
        "    # For each dominant color, draw a rectangle on the blank image\n",
        "    for color in colors:\n",
        "        endX = startX + (300 // len(colors))\n",
        "        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50), color.astype(int).tolist(), -1)\n",
        "        startX = endX\n",
        "\n",
        "    # Display the image\n",
        "    cv2_imshow(bar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4VtFhBqf-je"
      },
      "outputs": [],
      "source": [
        "def get_color_frequencies(img):\n",
        "    ''' Get color frequencies and return pd dataframe '''\n",
        "\n",
        "    img_nx3 = np.float32(img.reshape(-1, 3))\n",
        "\n",
        "    unique_pixels, counts = np.unique(img_nx3, axis=0, return_counts=True)\n",
        "\n",
        "    color_dict = {'color': [tuple(color) for color in unique_pixels],\n",
        "                  'frequency': counts}\n",
        "\n",
        "    return pd.DataFrame(color_dict)\n",
        "\n",
        "def get_most_distinctive_color(color_freq_home, color_freq_away, ratio=10):\n",
        "    ''' Given the color frequency per team, get the most distinctive color per team '''\n",
        "\n",
        "    df_merged = pd.merge(color_freq_home, color_freq_away, on='color', suffixes=('_df1', '_df2'), how='outer')\n",
        "    # NaN is filled with 1, which is not true but precise in our results\n",
        "    df_merged['frequency_df1'] = df_merged['frequency_df1'].fillna(1)\n",
        "    df_merged['frequency_df2'] = df_merged['frequency_df2'].fillna(1)\n",
        "\n",
        "    # Compute difference between frequencies in both dataframes\n",
        "    df_merged['frequency_difference'] = abs(df_merged['frequency_df1'] / df_merged['frequency_df2'])\n",
        "\n",
        "    # If the frequencies are bigger than ratio or lower than 1/ratio or NaN\n",
        "    # (meaning that the color is in one side, but not in the other) is considered\n",
        "    # a difference to consider\n",
        "\n",
        "    while True:\n",
        "        outstanding_differences = df_merged[\n",
        "            (df_merged['frequency_difference'] > ratio) |\n",
        "            (df_merged['frequency_difference'] < 1/ratio)\n",
        "        ]\n",
        "        # There has to be at least one color per team\n",
        "        if (outstanding_differences['frequency_difference'] > 1).any() and (outstanding_differences['frequency_difference'] < 1).any():\n",
        "            break\n",
        "        ratio /= 1.2\n",
        "\n",
        "    # Retrieve the rows with the most frequency for each df from these differences\n",
        "    max_frequency_df1 = outstanding_differences.loc[outstanding_differences['frequency_df1'].idxmax()]\n",
        "    max_frequency_df2 = outstanding_differences.loc[outstanding_differences['frequency_df2'].idxmax()]\n",
        "\n",
        "    return max_frequency_df1['color'], max_frequency_df2['color']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwNT01HB7UeW"
      },
      "outputs": [],
      "source": [
        "def get_color_per_team_from_video(filepath, kmeans_nclusters=32, color_freq_ratio=10, debug_color = False):\n",
        "    ''' Workflow which takes a video as input and return the color assigned to home and away team '''\n",
        "    # Extract scoreboard from video\n",
        "    background = get_static_pixels_from_video(filepath)\n",
        "    scoreboard = detect_team_scoreboard_and_crop_image(background)\n",
        "\n",
        "    # Quantize scoreboard and split in home-away teams\n",
        "    quantized_scoreboard = quantize_img(scoreboard, kmeans_nclusters)\n",
        "    quantized_home_scoreboard, quantized_away_scoreboard = split_scoreboard_per_team(quantized_scoreboard)\n",
        "\n",
        "    # Get color frequency per team\n",
        "    color_frequencies_home_scoreboard = get_color_frequencies(quantized_home_scoreboard)\n",
        "    color_frequencies_away_scoreboard = get_color_frequencies(quantized_away_scoreboard)\n",
        "\n",
        "    # Get the most used color in a team that is mostly never used in the other team\n",
        "    home_color, away_color = get_most_distinctive_color(color_frequencies_home_scoreboard, color_frequencies_away_scoreboard, color_freq_ratio)\n",
        "\n",
        "    if debug_color == True:\n",
        "        print('Static pixels in sample:')\n",
        "        cv2_imshow(background)\n",
        "        print('Detect scoreboard')\n",
        "        cv2_imshow(scoreboard)\n",
        "        print('Quantize scoreboard per team')\n",
        "        cv2_imshow(quantized_home_scoreboard)\n",
        "        cv2_imshow(quantized_away_scoreboard)\n",
        "        print('Most distinctive color per team')\n",
        "        display_colors([np.array(home_color), np.array(away_color)])\n",
        "\n",
        "    return home_color, away_color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MnmPm7xF4dr"
      },
      "source": [
        "##Color filtering based on team jerseys\n",
        "Once we have obtained from the scoreboard the color of the home team and the color of the away team, we use the OpenCV *cv2.inRange* method to detect the colors in the jerseys that are in a **range** around the colors of the teams.\n",
        "\n",
        "\n",
        "This process is performed in the **HSV color space**, as it allows a better color separation and to take into account aspects such as brightness and saturation, which are important for color detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K54C_-F8NzAh"
      },
      "outputs": [],
      "source": [
        "def get_color_range(color, h_range=20, s_range=90, v_range=90):\n",
        "    ''' Calculate a range of colors in the HSV color space based on a given RGB color and a range in HSV space'''\n",
        "    h, s, v= colorsys.rgb_to_hsv(color[2], color[1], color[0])\n",
        "\n",
        "    # translate hsv into opencv space\n",
        "    h = int(180*h)\n",
        "    s = int(255*s)\n",
        "    v = int(v)\n",
        "\n",
        "    low_h=h-h_range\n",
        "    top_h=h+h_range\n",
        "    additionalMask = False\n",
        "\n",
        "    # h coordinates are circular, we have to consider an additional mask in this case\n",
        "    if low_h < 0:\n",
        "        new_low_h = low_h + 180\n",
        "        new_top_h = 180\n",
        "        low_h = 0\n",
        "        additionalMask = True\n",
        "    elif top_h>180:\n",
        "        new_top_h = top_h - 180\n",
        "        new_low_h = 0\n",
        "        top_h = 180\n",
        "        additionalMask = True\n",
        "\n",
        "    lower_bound = np.array([low_h, 10, 10])\n",
        "    upper_bound = np.array([top_h, 255, 255])\n",
        "\n",
        "    if additionalMask:\n",
        "        additional_lower_bound = np.array([new_low_h, 10, 10])\n",
        "        additional_upper_bound = np.array([new_top_h, 255, 255])\n",
        "        return lower_bound, upper_bound, additional_lower_bound, additional_upper_bound\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "def get_mask(player_hsv, boundaries):\n",
        "    ''' Filter player via HSV mask '''\n",
        "    mask = cv2.inRange(player_hsv, boundaries[0], boundaries[1])\n",
        "\n",
        "    # In case there are two masks due to h being red or similar\n",
        "    if len(boundaries)==4:\n",
        "        mask1 = cv2.inRange(player_hsv, boundaries[2], boundaries[3])\n",
        "        mask = mask + mask1\n",
        "\n",
        "    return mask\n",
        "\n",
        "def team_classification(player, config_color, debug=False):\n",
        "    ''' Classify the team of a player based on the color distribution in the player image '''\n",
        "    player_hsv = cv2.cvtColor(player,cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Mask and count for home team color\n",
        "    mask_home_color = cv2.bitwise_and(player_hsv,player_hsv,mask=get_mask(player_hsv, config_color['home_color_range']))\n",
        "    count_mask_home_color = np.count_nonzero(mask_home_color)\n",
        "\n",
        "    # Mask and count for away team color\n",
        "    mask_away_color = cv2.bitwise_and(player_hsv,player_hsv,mask=get_mask(player_hsv, config_color['away_color_range']))\n",
        "    count_mask_away_color = np.count_nonzero(mask_away_color)\n",
        "\n",
        "    # Calculate percentages of home and away team colors\n",
        "    player_count = np.count_nonzero(player_hsv)\n",
        "    home_percentage = count_mask_home_color / player_count\n",
        "    away_percentage = count_mask_away_color / player_count\n",
        "\n",
        "    if debug:\n",
        "        cv2_imshow(player)\n",
        "        cv2_imshow(player_hsv)\n",
        "\n",
        "        print('Percentage of pixels home team : ', home_percentage)\n",
        "        cv2_imshow(mask_home_color)\n",
        "\n",
        "        print('Percentage of pixels away team  : ', away_percentage)\n",
        "        cv2_imshow(mask_away_color)\n",
        "\n",
        "\n",
        "    # Determine team classification based on color percentages\n",
        "    if home_percentage > 0.01 and home_percentage > away_percentage:\n",
        "        return 'Home', config_color['home_color']\n",
        "    elif away_percentage>0.01 and away_percentage > home_percentage:\n",
        "        return 'Away', config_color['away_color']\n",
        "    else:\n",
        "        return 'Not sure', (0.0, 0.0, 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FFNa-IFvDmx"
      },
      "outputs": [],
      "source": [
        "def process_ball(ball_img, ball_bounding_boxes):\n",
        "    '''Process the image of a ball and its bounding boxes'''\n",
        "    if ball_bounding_boxes:\n",
        "        # get most confident ball detected\n",
        "        box = ball_bounding_boxes[ball_bounding_boxes.conf.argmax()]\n",
        "        x, y, w, h = map(int, box.xywh.tolist()[0])\n",
        "        ball_img = cv2.rectangle(ball_img, (x-w//2, y-h//2), (x+w//2,y+h//2), (255,255,0), 2)\n",
        "        ball_img = cv2.putText(ball_img, \"Ball\",(x, y-h//2-10), cv2.FONT_HERSHEY_SIMPLEX, 0.50, (0, 0, 0), 2)\n",
        "        return ball_img, (x, y, w, h)\n",
        "    else:\n",
        "        return ball_img, None\n",
        "\n",
        "def process_persons(orig_img, bounding_boxes, config_color):\n",
        "    ''' Postprocess detected persons. If it is a player, assign it to a team. If it is a referee, just label it. '''\n",
        "\n",
        "    hsv = cv2.cvtColor(orig_img, cv2.COLOR_BGR2HSV)\n",
        "    final_image = np.copy(orig_img)\n",
        "\n",
        "    # Dictionary to count persons belonging to each team and referees\n",
        "    count_persons = {'home': 0, 'away': 0, 'referees': 0}\n",
        "\n",
        "    # Mask to remove green color (for field)\n",
        "    green_mask = cv2.inRange(hsv, (35, 35, 35), (70, 255,255))\n",
        "    inverted_green_mask= cv2.bitwise_not(green_mask)\n",
        "    img_without_green = cv2.bitwise_and(orig_img, orig_img, mask=inverted_green_mask)\n",
        "\n",
        "    for box in bounding_boxes:\n",
        "        # Box position and dimensions\n",
        "        x, y, w, h = map(int, box.xywh.tolist()[0])\n",
        "\n",
        "        # If the box corresponds to a person (cls 1)\n",
        "        if box.cls==1:\n",
        "            # Crop image to only have the torso\n",
        "            player_torso = img_without_green[y-h//2:y,x-w//2:x+w//2]\n",
        "\n",
        "            # Classify the team of the person\n",
        "            team_text, color_float = team_classification(player_torso, config_color)\n",
        "\n",
        "            # Process image\n",
        "            final_image = cv2.rectangle (final_image, (x-w//2, y-h//2), (x+w//2,y+h//2), (int(color_float[0]), int(color_float[1]), int(color_float[2])), 2)\n",
        "            final_image = cv2.putText(final_image, team_text + ' ID:'+str(int(box.id.item())), (x-w//2, y-h//2-10), cv2.FONT_HERSHEY_SIMPLEX, 0.50, (0, 0, 0), 2)\n",
        "\n",
        "            # Update person count based on team classification\n",
        "            if team_text == 'Home':\n",
        "                count_persons['home'] += 1\n",
        "            elif team_text == 'Away':\n",
        "                count_persons['away'] += 1\n",
        "\n",
        "        # If the box corresponds to a referee (cls 2)\n",
        "        elif box.cls==2:\n",
        "            # Process image\n",
        "            final_image = cv2.rectangle (final_image, (x-w//2, y-h//2), (x+w//2,y+h//2), (0,0,0), 2)\n",
        "            final_image = cv2.putText(final_image,'Referee', (x-w//2, y-h//2-10), cv2.FONT_HERSHEY_SIMPLEX, 0.50, (0, 0, 0), 2)\n",
        "\n",
        "            # Update referee count\n",
        "            count_persons['referees'] += 1\n",
        "\n",
        "    return final_image, count_persons"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSONL"
      ],
      "metadata": {
        "id": "goplyWtftlB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json_output_per_frame(frame, count_persons, ball_location, output_each_n_frames=5):\n",
        "    ''' Ordered dictionary based on requeriment '''\n",
        "    return OrderedDict([('frame', frame), ('home_team', count_persons['home']), ('away_team', count_persons['away']), ('refs', count_persons['referees']), ('ball_loc', ball_location)])"
      ],
      "metadata": {
        "id": "zkFrwch-tmPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_output_json(filename, json_output):\n",
        "    ''' Creates jsonl based on a dictionary list '''\n",
        "    with jsonlines.open(filename,'w') as writer:\n",
        "        for elem in json_output:\n",
        "            writer.write(elem)\n"
      ],
      "metadata": {
        "id": "ilT5NvwN0Jh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNOOYedWo4IR"
      },
      "source": [
        "## Players detection and classification\n",
        "The last step of this process would be to run through the video clip we want to process and use the **YOLO model** to obtain the player detections."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def match_detection_pipeline(config):\n",
        "    ''' Perform object detection using YOLO model.\n",
        "        Obtain team color by locating the scoreboard in the image.\n",
        "        Assign a team to a player based on the color.\n",
        "        Generate video and json output. '''\n",
        "\n",
        "    time_init = time.time()\n",
        "\n",
        "    # Init parameters\n",
        "    json_output = []\n",
        "    frame_index = 0\n",
        "    model = YOLO(config['model_checkpoint_path'])\n",
        "\n",
        "    # Get input video and create output video\n",
        "    input_video = cv2.VideoCapture(config['input_video_path'])\n",
        "\n",
        "    width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(input_video.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    output_video = cv2.VideoWriter(config['output_video_path'], cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    # Get most distinctive color per team from input video\n",
        "    home_color, away_color = get_color_per_team_from_video(config['input_video_path'], debug_color = config['debug_colors'])\n",
        "\n",
        "    # Get color range to identify each player team\n",
        "    home_color_range = get_color_range(home_color)\n",
        "    away_color_range = get_color_range(away_color)\n",
        "\n",
        "    config['color'] = {'home_color': home_color, 'home_color_range': home_color_range, 'away_color': away_color, 'away_color_range': away_color_range}\n",
        "\n",
        "    time_preprocessing = time.time()\n",
        "    time_list_inference = []\n",
        "    time_list_postprocessing = []\n",
        "\n",
        "    while input_video.isOpened():\n",
        "        # Read each frame\n",
        "        success, frame = input_video.read()\n",
        "\n",
        "        time_loop_init = time.time()\n",
        "        if success:\n",
        "            # Persons have persist = True so the idenfier doesn't change\n",
        "            results_persons = inference_video(model, frame, persist=True, classes=[config['class_player_index'], config['class_referee_index']])[0]\n",
        "            # Ball have persist = False since the detection per frame is better this way\n",
        "            results_ball = inference_video(model, frame, persist=False, classes=[config['class_ball_index']])[0]\n",
        "\n",
        "            time_loop_inference = time.time()\n",
        "            # Post process detections\n",
        "            processed_img, ball_coords = process_ball(results_ball.orig_img, results_ball.boxes)\n",
        "            new_frame, count_persons = process_persons(processed_img, results_persons.boxes, config['color'])\n",
        "\n",
        "            # Frame video output\n",
        "            output_video.write(new_frame)\n",
        "\n",
        "            # Frame JSON output\n",
        "            if frame_index % 5 == 0:\n",
        "                json_output.append(get_json_output_per_frame(frame_index, count_persons, ball_coords))\n",
        "\n",
        "            time_loop_postprocessing = time.time()\n",
        "            time_list_inference.append(time_loop_inference - time_loop_init)\n",
        "            time_list_postprocessing.append(time_loop_postprocessing - time_loop_inference)\n",
        "\n",
        "            frame_index += 1\n",
        "\n",
        "            #cv2_imshow(new_frame)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    time_end = time.time()\n",
        "    write_output_json(config['output_json_path'], json_output)\n",
        "\n",
        "    input_video.release()\n",
        "    output_video.release()\n",
        "\n",
        "    computational_time = {}\n",
        "    computational_time['preprocessing'] = round(time_preprocessing - time_init, 2)\n",
        "    computational_time['inference'] = round(sum(time_list_inference), 2)\n",
        "    computational_time['postprocessing'] = round(sum(time_list_postprocessing), 2)\n",
        "    computational_time['inference_mean'] = round(sum(time_list_inference) / len(time_list_inference), 2)\n",
        "    computational_time['postprocessing_mean'] = round(sum(time_list_postprocessing) / len(time_list_postprocessing), 2)\n",
        "    computational_time['total'] = round(time_end - time_init, 2)\n",
        "\n",
        "    return computational_time"
      ],
      "metadata": {
        "id": "YLz4Pf_jczku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "NVPsuzmVdANl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    config = {}\n",
        "    config['debug_colors'] = False\n",
        "    config['output_each_n_frames'] = 5\n",
        "    config['class_ball_index'] = 0\n",
        "    config['class_player_index'] = 1\n",
        "    config['class_referee_index'] = 2\n",
        "\n",
        "    config['model_checkpoint_path'] = '/content/football-task-delivery/yolov8n-1088p-motionblur-6xball3xreferee/weights/best.pt'\n",
        "\n",
        "    for i in range(1,4):\n",
        "        print(f'Starting clip_{i} ...')\n",
        "        config['input_video_path'] = f'/content/football-task-delivery/clip_{i}.mp4'\n",
        "        config['output_video_path'] = f'/content/football-task-delivery/output/clip_{i}_output.mp4'\n",
        "        config['output_json_path'] = f'/content/football-task-delivery/output/clip_{i}_output.json'\n",
        "\n",
        "        computational_time = match_detection_pipeline(config)\n",
        "\n",
        "        print('\\n')\n",
        "        print(f'Total time for clip_{i}: ' + str(computational_time['total']) + ' seconds')\n",
        "        print('Preprocessing time: ' + str(computational_time['preprocessing']) + ' seconds')\n",
        "        print('Inference total time: ' + str(computational_time['inference']) + ' seconds')\n",
        "        print('Inference mean time per frame: ' + str(computational_time['inference_mean']) + ' seconds')\n",
        "        print('Postprocessing total time: ' + str(computational_time['postprocessing']) + ' seconds')\n",
        "        print('Postprocessing mean time per frame: ' + str(computational_time['postprocessing_mean']) + ' seconds')\n",
        "        print('\\n')\n"
      ],
      "metadata": {
        "id": "BLmy21QMWjqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBdQTMNG8sUY",
        "outputId": "87c14c68-93f9-45f9-b9f5-6c6177bf6b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting clip_1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
            "\n",
            "Total time for clip_1: 84.21 seconds\n",
            "Preprocessing time: 19.65 seconds\n",
            "Inference total time: 46.91 seconds\n",
            "Inference mean time per frame: 0.09 seconds\n",
            "Postprocessing total time: 16.29 seconds\n",
            "Postprocessing mean time per frame: 0.03 seconds\n",
            "\n",
            "\n",
            "Starting clip_2 ...\n",
            "\n",
            "\n",
            "Total time for clip_2: 44.07 seconds\n",
            "Preprocessing time: 9.01 seconds\n",
            "Inference total time: 24.85 seconds\n",
            "Inference mean time per frame: 0.09 seconds\n",
            "Postprocessing total time: 9.48 seconds\n",
            "Postprocessing mean time per frame: 0.04 seconds\n",
            "\n",
            "\n",
            "Starting clip_3 ...\n",
            "\n",
            "\n",
            "Total time for clip_3: 48.66 seconds\n",
            "Preprocessing time: 9.91 seconds\n",
            "Inference total time: 27.98 seconds\n",
            "Inference mean time per frame: 0.09 seconds\n",
            "Postprocessing total time: 9.99 seconds\n",
            "Postprocessing mean time per frame: 0.03 seconds\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning"
      ],
      "metadata": {
        "id": "jTPzZWjZzSa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training = False\n",
        "model_checkpoint = 'yolov8n.pt'\n",
        "model = YOLO(model_checkpoint)"
      ],
      "metadata": {
        "id": "lB2PAFyvzWHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d151c7-6f99-4a57-e2c5-6f1a81a99b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.23M/6.23M [00:00<00:00, 271MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if training:\n",
        "    # Paste the yaml dataset location in the path\n",
        "    finetune_model(model, '/content/dataset/data.yaml', pos_weight=[6.0, 1.0, 3.0], imgsz=1088)"
      ],
      "metadata": {
        "id": "dRYwnrgfzcBF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}